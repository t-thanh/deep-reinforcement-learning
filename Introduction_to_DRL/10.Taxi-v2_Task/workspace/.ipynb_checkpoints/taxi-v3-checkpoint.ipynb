{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.1.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/nttputus/Python3evns/dspy/lib/python3.6/site-packages (from bayesian-optimization) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /home/nttputus/Python3evns/dspy/lib/python3.6/site-packages (from bayesian-optimization) (0.19.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/nttputus/Python3evns/dspy/lib/python3.6/site-packages (from bayesian-optimization) (1.13.3)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000/20000 || Best average reward 8.843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "agent = Agent(epsilon=0.1, alpha=0.1, gamma=0.9)\n",
    "avg_rewards, best_avg_reward = interact(env, agent, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_wrapper(epsilon, alpha, gamma):\n",
    "    agent = Agent(epsilon=epsilon, alpha=alpha, gamma=gamma)\n",
    "    avg_rewards, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |  epsilon  |   gamma   |\n",
      "-------------------------------------------------------------\n",
      "Episode 20000/20000 || Best average reward 9.141\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 9.14    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.678\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 8.67    \u001b[0m | \u001b[0m 0.1454  \u001b[0m | \u001b[0m 0.0977  \u001b[0m | \u001b[0m 0.8644  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.818\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 8.81    \u001b[0m | \u001b[0m 0.2406  \u001b[0m | \u001b[0m 0.07368 \u001b[0m | \u001b[0m 0.8998  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.894\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 8.8     \u001b[0m | \u001b[0m 0.3582  \u001b[0m | \u001b[0m 0.04731 \u001b[0m | \u001b[0m 0.853   \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.712\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 8.71    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.215\n",
      "\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 9.21    \u001b[0m | \u001b[95m 0.4049  \u001b[0m | \u001b[95m 0.08995 \u001b[0m | \u001b[95m 0.9427  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.169\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 9.16    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.721\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 8.72    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.616\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 8.61    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.742\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 8.74    \u001b[0m | \u001b[0m 0.4691  \u001b[0m | \u001b[0m 0.08741 \u001b[0m | \u001b[0m 0.5261  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.945\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 8.94    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.7679  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.755\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 8.75    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.625\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 8.62    \u001b[0m | \u001b[0m 0.1534  \u001b[0m | \u001b[0m 0.08277 \u001b[0m | \u001b[0m 0.7767  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.754\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 8.75    \u001b[0m | \u001b[0m 0.2785  \u001b[0m | \u001b[0m 0.09936 \u001b[0m | \u001b[0m 0.5026  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.959\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 8.9     \u001b[0m | \u001b[0m 0.4415  \u001b[0m | \u001b[0m 0.01507 \u001b[0m | \u001b[0m 0.5809  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.778\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 8.77    \u001b[0m | \u001b[0m 0.3053  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.827\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 8.82    \u001b[0m | \u001b[0m 0.3388  \u001b[0m | \u001b[0m 0.04    \u001b[0m | \u001b[0m 0.9435  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.841\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 8.84    \u001b[0m | \u001b[0m 0.2526  \u001b[0m | \u001b[0m 0.09438 \u001b[0m | \u001b[0m 0.5533  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.751\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 8.75    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.8589  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.729\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 8.72    \u001b[0m | \u001b[0m 0.1703  \u001b[0m | \u001b[0m 0.03079 \u001b[0m | \u001b[0m 0.6747  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.647\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 8.64    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.6136  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.644\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 8.64    \u001b[0m | \u001b[0m 0.2814  \u001b[0m | \u001b[0m 0.07075 \u001b[0m | \u001b[0m 0.756   \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.716\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 8.71    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.815\n",
      "\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 8.81    \u001b[0m | \u001b[0m 0.2703  \u001b[0m | \u001b[0m 0.03413 \u001b[0m | \u001b[0m 0.6746  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.069\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 0.4627  \u001b[0m | \u001b[0m 0.09269 \u001b[0m | \u001b[0m 0.9147  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.785\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 8.7     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7029  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.021\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 9.02    \u001b[0m | \u001b[0m 0.3638  \u001b[0m | \u001b[0m 0.07474 \u001b[0m | \u001b[0m 0.6848  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.667\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 8.66    \u001b[0m | \u001b[0m 0.3155  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.874\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 8.87    \u001b[0m | \u001b[0m 0.3239  \u001b[0m | \u001b[0m 0.04467 \u001b[0m | \u001b[0m 0.9427  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {'epsilon': (0.01, 0.1), 'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=interact_wrapper,\n",
    "    pbounds=pbounds,\n",
    "    random_state=47\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
